## This script was auto-generated by using
## % comet init

## For more information, please see:
## https://www.comet.ml/docs/command-line/#comet-init

import comet_ml
{%- if cookiecutter.confusion_matrix == "Yes" %}
from comet_ml import ConfusionMatrix
{%- endif %}

import numpy as np
import random
import logging
LOGGER = logging.getLogger("comet_ml")

{%- if cookiecutter.framework == 'keras' %}
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.datasets import mnist
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.utils import to_categorical
{%- endif %}

{%- if cookiecutter.framework == 'pytorch' %}
import torch
import torch.nn as nn
import torchvision.datasets as dsets
import torchvision.transforms as transforms
from torch.autograd import Variable
{%- endif %}

{%- if cookiecutter.embedding == "Yes" %}
import keract
{%- endif %}


def get_comet_experiment(**args):
    {%- if cookiecutter.online_or_offline == "Online" %}
    return comet_ml.Experiment(**args)
    {%- elif cookiecutter.online_or_offline == "Offline" %}
    return comet_ml.OfflineExperiment(**args)
    {%- endif %}


{%- if cookiecutter.framework == 'keras' %}


def build_model(experiment):
    model = Sequential()
    model.add(
        Dense(
            experiment.get_parameter("first_layer_units"),
            activation="sigmoid",
            input_shape=(784,),
        )
    )
    model.add(Dense(experiment.get_parameter("hidden_size"), activation="sigmoid"))
    model.add(Dense(experiment.get_parameter("hidden_size"), activation="sigmoid"))
    model.add(Dense(
        experiment.get_parameter("num_classes"),
        activation="softmax"))
    model.compile(
        loss="categorical_crossentropy",
        optimizer=RMSprop(),
        metrics=["accuracy"],
    )

    return model


def train(experiment, model, datasets):
    x_train, y_train, x_test, y_test = datasets
    input_size = experiment.get_parameter("input_size")

    {%- if cookiecutter.confusion_matrix == "Yes" %}
    class ConfusionMatrixCallback(Callback):
        def __init__(self, experiment, inputs, targets):
            self.experiment = experiment
            self.inputs = inputs
            self.targets = targets
            # We make a ConfusionMatrix here so that we can share
            # examples across epochs and cut down on the images
            # uploaded:
            self.confusion_matrix = ConfusionMatrix(
                index_to_example_function=self.index_to_example)

        def on_epoch_begin(self, epoch, logs={}):
            # Get Confusion Matrix before training:
            if epoch == 0:
                self.on_epoch_end(-1)

        def on_epoch_end(self, epoch, logs={}):
            predicted = self.model.predict(self.inputs)
            # First we compute the matrix:
            self.confusion_matrix.compute_matrix(self.targets, predicted)
            # Then log it from the ConfusionMatrix object:
            self.experiment.log_confusion_matrix(
                matrix=self.confusion_matrix,
                title="Confusion Matrix, Epoch #%d" % (epoch + 1),
                file_name="confusion-matrix-%03d.json" % (epoch + 1),
            )

        def index_to_example(self, index):
            image_array = self.inputs[index]
            image_name = "confusion-matrix-%05d.png" % index
            result = self.experiment.log_image(
                image_array,
                name=image_name,
                image_shape=(input_size, input_size, 1))
            # Return sample name and assetId
            return {"sample": image_name, "assetId": result["imageId"]}

    {%- endif %}

    {%- if cookiecutter.embedding == "Yes" %}
    class EmbeddingCallback(Callback):
        def __init__(self, experiment, inputs, targets):
            self.experiment = experiment
            self.inputs = inputs
            self.targets = targets
            self.labels = targets.argmax(axis=1)
            self.image_size = (input_size, input_size)
            results = self.experiment.create_embedding_image(
                image_data=self.inputs,
                # we round the pixels to 0 and 1, and multiple by 2
                # to keep the non-zero colors dark (if they were 1, they
                # would get distributed between 0 and 255):
                image_preprocess_function=lambda matrix: np.round(matrix,0) * 2,
                # Set the transparent color:
                image_transparent_color=(0, 0, 0),
                image_size=self.image_size,
                # Fill in the transparent color with a background color:
                image_background_color_function=self.label_to_color,
            )
            if results:
                image, self.sprite_url = results
            else:
                self.sprite_url = None

        def label_to_color(self, index):
            label = self.labels[index]
            if label == 0:
                return (255, 0, 0)
            elif label == 1:
                return (0, 255, 0)
            elif label == 2:
                return (0, 0, 255)
            elif label == 3:
                return (255, 255, 0)
            elif label == 4:
                return (0, 255, 255)
            elif label == 5:
                return (128, 128, 0)
            elif label == 6:
                return (0, 128, 128)
            elif label == 7:
                return (128, 0, 128)
            elif label == 8:
                return (255, 0, 255)
            elif label == 9:
                return (255, 255, 255)

        def on_epoch_begin(self, epoch, logs={}):
            # Get Embedding before training:
            if epoch == 0:
                self.on_epoch_end(-1)

        def on_epoch_end(self, epoch, logs={}):
            # The vectors are big, only log them every 10 epochs:
            if (epoch + 1) % 10 != 0:
                return
            # Assuming one input bank:
            if self.model:
                input_tensor = self.inputs
                try:
                    activations = keract.get_activations(self.model, input_tensor)
                except Exception:
                    LOGGER.warning("keract.get_activations() failed... try upgrading tensorflow and keract; ignoring embeddings")
                    return
                keys = list(activations.keys())
                # Get the activations before output layer:
                if len(keys) >= 2:
                    layer_name = keys[-2]
                    # Group the embeddings by layer_name:
                    self.experiment.log_embedding(
                        activations[layer_name],
                        self.labels,
                        image_data=self.sprite_url,
                        image_size=self.image_size,
                        title="%s-%s" % (layer_name, epoch + 1),
                        group=layer_name)

    {%- endif %}

    callbacks = [
        {%- if cookiecutter.confusion_matrix == "Yes" %}
        ConfusionMatrixCallback(experiment, x_test, y_test),
        {%- endif %}
        {%- if cookiecutter.embedding == "Yes" and cookiecutter.online_or_offline == "Online"%}
        EmbeddingCallback(experiment, x_test, y_test),
        {%- endif %}
    ]

    {%- if cookiecutter.embedding == "Yes" and cookiecutter.online_or_offline == "Offline"%}
    LOGGER.warning("Embedding is currently only supported for online experiments")
    {%- endif %}

    model.fit(
        x_train,
        y_train,
        batch_size=experiment.get_parameter("batch_size"),
        epochs=experiment.get_parameter("epochs"),
        validation_data=(x_test, y_test),
        callbacks=callbacks,
    )

def evaluate(experiment, model, datasets):
    x_train, y_train, x_test, y_test = datasets

    score = model.evaluate(x_test, y_test, verbose=0)
    LOGGER.info("Score %s", score)

def get_datasets(hyper_params):
    # the data, shuffled and split between train and test sets
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    x_train = x_train.reshape(60000, 784)
    x_test = x_test.reshape(10000, 784)
    x_train = x_train.astype("float32")
    x_test = x_test.astype("float32")
    x_train /= 255
    x_test /= 255
    print(x_train.shape[0], "train samples")
    print(x_test.shape[0], "test samples")

    # convert class vectors to binary class matrices
    y_train = to_categorical(y_train, hyper_params["num_classes"])
    y_test = to_categorical(y_test, hyper_params["num_classes"])

    return x_train, y_train, x_test, y_test


{%- endif %}

{%- if cookiecutter.framework == 'pytorch' %}

def train(experiment, model, datasets):
    from torch.utils.data import SubsetRandomSampler

    rnn, criterion, optimizer = model
    train_dataset, test_dataset = datasets

    # Data Loader (Input Pipeline)
    # Sampling only from first 1000
    sampler = SubsetRandomSampler(list(range(1000)))
    train_loader = torch.utils.data.DataLoader(
        dataset=train_dataset,
        batch_size=experiment.get_parameter('batch_size'),
        sampler=sampler,
        #shuffle=True,
    )

    {%- if cookiecutter.confusion_matrix == "Yes" %}
    def index_to_example(index):
        image_array, label, _index = train_dataset[index]
        image_name = "confusion-matrix-%05d.png" % index
        result = experiment.log_image(
            image_array,
            name=image_name,
            image_shape=(28, 28, 1))
        #Return sample name and assetId
        return {"sample": image_name, "assetId": result["imageId"]}

    # We make a ConfusionMatrix here so that we can share
    # examples across epochs and cut down on the images
    # uploaded:
    confusion_matrix = ConfusionMatrix(
        index_to_example_function=index_to_example)

    {%- endif %}

    # In order:
    print("Collecting 1000 data...")
    all_labels = [label for (image, label, index) in train_dataset if index < 1000]
    inputs = [image.numpy() for (image, label, index) in train_dataset if index < 1000]
    sprite_url = None
    with experiment.train():
        step = 0
        for epoch in range(experiment.get_parameter('epochs')):
            correct = 0
            total = 0
            batch_actual = []
            batch_predicted = []
            batch_indices = []
            for batch_step, (images, labels, indices) in enumerate(train_loader):
                print("batch step:", batch_step, "...")
                images = Variable(images.view(
                    -1,
                    experiment.get_parameter('sequence_length'),
                    experiment.get_parameter("input_size")))

                labels = Variable(labels)

                # Forward + Backward + Optimize
                optimizer.zero_grad()
                outputs = rnn(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()

                # Compute train accuracy
                _, predicted = torch.max(outputs.data, 1)
                batch_total = labels.size(0)
                total += batch_total

                batch_correct = (predicted == labels.data).sum()
                correct += batch_correct

                batch_predicted.extend(predicted.numpy().tolist())
                batch_actual.extend(labels.data.numpy().tolist())
                batch_indices.extend(indices.numpy().tolist())

                # Log batch_accuracy to Comet.ml; step is each batch
                step += 1
                experiment.log_metric("batch_accuracy", batch_correct / batch_total, step=step)

                if (batch_step + 1) % 100 == 0:
                    print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'
                          % (epoch + 1,
                             experiment.get_parameter('epochs'),
                             batch_step + 1,
                             len(train_dataset) // experiment.get_parameter('batch_size'),
                             loss.item()))

            # Log epoch accuracy to Comet.ml; step is each epoch
            experiment.log_metric("batch_accuracy", correct / total, step=epoch, epoch=epoch)

            {%- if cookiecutter.confusion_matrix == "Yes" %}

            indices, actual, predicted = list(zip(*sorted(zip(
                batch_indices, batch_actual, batch_predicted))))

            print("Creating confusion matrix...")
            confusion_matrix.compute_matrix(actual, predicted)

            experiment.log_confusion_matrix(
                matrix=confusion_matrix,
                title="MNIST Confusion Matrix, Epoch #%d" % (epoch + 1),
                file_name="confusion-matrix-%03d.json" % (epoch + 1),
            )
            {%- endif %}

            # Epoch level items
            {%- if cookiecutter.embedding == "Yes" %}

            if epoch == 0:
                # First, create the embedding image:
                def label_to_color(index):
                    label = all_labels[index]
                    if label == 0:
                        return (255, 0, 0)
                    elif label == 1:
                        return (0, 255, 0)
                    elif label == 2:
                        return (0, 0, 255)
                    elif label == 3:
                        return (255, 255, 0)
                    elif label == 4:
                        return (0, 255, 255)
                    elif label == 5:
                        return (128, 128, 0)
                    elif label == 6:
                        return (0, 128, 128)
                    elif label == 7:
                        return (128, 0, 128)
                    elif label == 8:
                        return (255, 0, 255)
                    elif label == 9:
                        return (255, 255, 255)

                print("Creating embedding image...")
                results = experiment.create_embedding_image(
                    image_data=inputs,
                    # we round the pixels to 0 and 1, and multiple by 2
                    # to keep the non-zero colors dark (if they were 1, they
                    # would get distributed between 0 and 255):
                    image_preprocess_function=lambda matrix: np.round(matrix,0) * 2,
                    # Set the transparent color:
                    image_transparent_color=(0, 0, 0),
                    image_size=(
                        experiment.get_parameter("input_size"),
                        experiment.get_parameter("input_size"),
                    ),
                    # Fill in the transparent color with a background color:
                    image_background_color_function=label_to_color,
                )
                if results:
                    image, sprite_url = results
                else:
                    sprite_url = None
            else:
                # In subsequent epochs, log the activations:
                if sprite_url:
                    print("Logging embeddings...")
                    activations = []
                    for i in range(len(all_labels)):
                        v = Variable(train_dataset[i][0])
                        output = rnn(v)
                        activations.append(rnn.out[0].detach().numpy())
                    layer_name = "activations/0"
                    experiment.log_embedding(
                        activations,
                        all_labels,
                        image_data=sprite_url,
                        image_size=(
                            experiment.get_parameter("input_size"),
                            experiment.get_parameter("input_size"),
                        ),
                        title="%s-%s" % (layer_name, epoch + 1),
                        group=layer_name)

            {%- endif %}
            {%- if cookiecutter.weight_histograms == "Yes" %}
            # TODO: log weight histograms here!
            {%- endif %}
            {%- if cookiecutter.activation_histograms == "Yes" %}
            # TODO: log activation histograms here!
            {%- endif %}
            {%- if cookiecutter.gradient_histograms == "Yes" %}
            # TODO: log gradient histograms here!
            {%- endif %}


def build_model(experiment):
    input_size = experiment.get_parameter("input_size")
    hidden_size = experiment.get_parameter("hidden_size")
    num_layers = experiment.get_parameter("num_layers")
    num_classes = experiment.get_parameter("num_classes")
    learning_rate = experiment.get_parameter("learning_rate")

    class RNN(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes):
            super(RNN, self).__init__()
            self.hidden_size = hidden_size
            self.num_layers = num_layers
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            # Set initial states
            h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))
            c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))

            # Forward propagate RNN
            self.out, _ = self.lstm(x, (h0, c0))

            # Decode hidden state of last time step
            out = self.fc(self.out[:, -1, :])
            return out

    rnn = RNN(
        input_size,
        hidden_size,
        num_layers,
        num_classes,
    )

    # Loss and Optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)

    return (rnn, criterion, optimizer)


def evaluate(experiment, model, datasets):
    rnn, criterion, optimizer = model
    train_dataset, test_dataset = datasets

    test_loader = torch.utils.data.DataLoader(
        dataset=test_dataset,
        batch_size=experiment.get_parameter('batch_size'),
        shuffle=False)

    with experiment.test():
        # Test the Model
        correct = 0
        total = 0
        for images, labels, index in test_loader:
            images = Variable(images.view(
                -1,
                experiment.get_parameter('sequence_length'),
                experiment.get_parameter("input_size")))
            outputs = rnn(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum()

        experiment.log_metric("accuracy", correct / total)
        print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))


def get_datasets(hyper_params):
    # MNIST Dataset
    def indexed_dataset(cls):
        def __getitem__(self, index):
            data, target = cls.__getitem__(self, index)
            return data, target, index

        return type(cls.__name__, (cls,), {
            '__getitem__': __getitem__,
        })

    MNIST = indexed_dataset(dsets.MNIST)

    train_dataset = MNIST(root='./data/',
                          train=True,
                          transform=transforms.ToTensor(),
                          download=True)

    test_dataset = MNIST(root='./data/',
                         train=False,
                         transform=transforms.ToTensor())

    return (train_dataset, test_dataset)

{%- endif %}


def main():
    hyper_params = {
        "epochs": {{ cookiecutter.epochs }},
        "batch_size": 120,
        "first_layer_units": 128,
        "sequence_length": 28,
        "input_size": 28,
        "hidden_size": 128,
        "num_layers": 2,
        "num_classes": 10,
        "learning_rate": 0.01
    }

    experiment_kwargs = {
        "project_name": "{{ cookiecutter.project_slug }}",
        {%- if cookiecutter.online_or_offline == "Offline" %}
        "offline_directory": ".",
        {%- endif %}
        {%- if cookiecutter.weight_histograms == "Yes" %}
        "auto_histogram_weight_logging": True,
        {%- endif %}
        {%- if cookiecutter.gradient_histograms == "Yes" %}
        "auto_histogram_gradient_logging": True,
        {%- endif %}
        {%- if cookiecutter.activation_histograms == "Yes" %}
        "auto_histogram_activation_logging": True,
        {%- endif %}
    }

    datasets = get_datasets(hyper_params)


    {%- if ((cookiecutter.weight_histograms == "Yes") or
            (cookiecutter.gradient_histograms == "Yes") or
            (cookiecutter.gradient_histograms == "Yes")) %}
    {%- if cookiecutter.framework == 'keras' %}
    ########################################################
    # If you would like to control exactly which histograms
    # are logged, how often, which inputs are used, and
    # to which layers, change these configuration variables:
    #
    # import os
    # os.environ["COMET_KERAS_HISTOGRAM_NAME_PREFIX"] = "{layer_num:0{max_digits}d}"
    # os.environ["COMET_KERAS_HISTOGRAM_ACTIVATION_INDEX_LIST"] = "0"
    # os.environ["COMET_KERAS_HISTOGRAM_ACTIVATION_LAYER_LIST"] = ""
    # os.environ["COMET_KERAS_HISTOGRAM_BATCH_SIZE"] = "1000"
    # os.environ["COMET_KERAS_HISTOGRAM_GRADIENT_INDEX_LIST"] = ""
    # os.environ["COMET_KERAS_HISTOGRAM_GRADIENT_LAYER_LIST"] = "-1"
    # os.environ["COMET_AUTO_LOG_HISTOGRAM_EPOCH_RATE"] = "1"
    #
    # For more information, see:
    # https://www.comet.ml/docs/python-sdk/advanced/#comet-environment-only-variables
    ########################################################
    {%- endif %}
    {%- endif %}

    {%- if cookiecutter.optimizer == 'No' %}

    experiment = get_comet_experiment(**experiment_kwargs)
    # Logging hyperparameters:
    experiment.log_parameters(hyper_params)

    {%- if cookiecutter.logging_samples == 'Yes' %}
    # Logging samples:
    experiment.log_text("example text 1")
    experiment.log_text("example text 2")
    experiment.log_text("example text 3")
    experiment.log_curve("curve 1",
                         x=[x for x in range(100)],
                         y=[y ** 2 for y in range(100)])
    experiment.log_points_3d("scene 1",
                         points=[(random.random(),
                                  random.random(),
                                  random.random()) for _ in range(100)],
    )
    {%- endif %}

    model = build_model(experiment)

    train(experiment, model, datasets)

    evaluate(experiment, model, datasets)

    experiment.end()
    {%- endif %}

    {%- if cookiecutter.optimizer == "Yes" %}
    config = {
        "algorithm": "bayes",
        "name": "Optimize MNIST Network",
        "spec": {
            "maxCombo": 10,
            "objective":
            "minimize",
{%- if cookiecutter.framework == 'pytorch' %}
            "metric": "train_loss",
{%- elif cookiecutter.framework == 'keras' %}
            "metric": "loss",
{%- endif %}
        },
        "parameters": {
            "first_layer_units": {
                "type": "integer",
                "mu": 500,
                "sigma": 50,
                "scalingType": "normal",
            },
            "batch_size": {"type": "discrete", "values": [64, 128, 256]},
        },
        "trials": 1,
    }

    {%- if cookiecutter.online_or_offline == "Online" %}
    experiment_class = comet_ml.Experiment
    {%- elif cookiecutter.online_or_offline == "Offline" %}
    experiment_class = comet_ml.OfflineExperiment
    {%- endif %}

    opt = comet_ml.Optimizer(config, experiment_class=experiment_class)

    for experiment in opt.get_experiments(**experiment_kwargs):
        # We remove the two hyperparameters that are set by optimizer
        # as we don't want to overwrite them:
        del hyper_params["first_layer_units"]
        del hyper_params["batch_size"]
        # Log the remainder:
        experiment.log_parameters(hyper_params)

        model = build_model(experiment)

        train(experiment, model, datasets)

        evaluate(experiment, model, datasets)

        experiment.end()
    {%- endif %}


if __name__ == "__main__":
    main()
